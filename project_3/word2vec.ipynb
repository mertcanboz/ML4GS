{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.12"},"colab":{"name":"word2vec.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"B_5n5Qj7Ujvu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":161},"executionInfo":{"status":"ok","timestamp":1593950988120,"user_tz":-120,"elapsed":34530,"user":{"displayName":"Clara Leung","photoUrl":"","userId":"02206143411713870524"}},"outputId":"183dfe36-d4ce-4cb8-beb7-7e2630992669"},"source":["## for colab only\n","from google.colab import drive \n","drive.mount(\"/content/drive\", force_remount=True)\n","%cd \"drive/My Drive/Colab Notebooks/MLGS/project3\"\n","%pwd"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/My Drive/Colab Notebooks/MLGS/project3\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic":{"type":"string"},"text/plain":["'/content/drive/My Drive/Colab Notebooks/MLGS/project3'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"VNbpr8hH_GB_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593420396015,"user_tz":-120,"elapsed":1096,"user":{"displayName":"Clara Leung","photoUrl":"","userId":"02206143411713870524"}},"outputId":"37ccb7ea-83cc-4a88-cb6d-90836e758bb6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fGvoHhq-T24j","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","from scipy import spatial\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mUQCobF_T24w","colab_type":"text"},"source":["# Project 3: Word2Vec (70 pt)\n","The goal of this project is to obtain the vector representations for words from text.\n","\n","The main idea is that words appearing in similar contexts have similar meanings. Because of that, word vectors of similar words should be close together. Models that use word vectors can utilize these properties, e.g., in sentiment analysis a model will learn that \"good\" and \"great\" are positive words, but will also generalize to other words that it has not seen (e.g. \"amazing\") because they should be close together in the vector space.\n","\n","Vectors can keep other language properties as well, like analogies. The question \"a is to b as c is to ...?\", where the answer is d, can be answered by looking into word vector space and calculating $\\mathbf{u}_b - \\mathbf{u}_a + \\mathbf{u}_c$, and finding the word vector that is the closest to the result.\n","\n","## Your task\n","Complete the missing code in this notebook. Make sure that all the functions follow the provided specification, i.e. the output of the function exactly matches the description in the docstring. \n","\n","We are given a text that contains $N$ unique words $\\{ x_1, ..., x_N \\}$. We will focus on the Skip-Gram model in which the goal is to predict the context window $S = \\{ x_{i-l}, ..., x_{i-1}, x_{i+1}, ..., x_{i+l} \\}$ from current word $x_i$, where $l$ is the window size. \n","\n","We get a word embedding $\\mathbf{u}_i$ by multiplying the matrix $\\mathbf{U}$ with a one-hot representation $\\mathbf{x}_i$ of a word $x_i$. Then, to get output probabilities for context window, we multiply this embedding with another matrix $\\mathbf{V}$ and apply softmax. The objective is to minimize the loss: $-\\mathop{\\mathbb{E}}[P(S|x_i;\\mathbf{U}, \\mathbf{V})]$.\n","\n","You are given a dataset with positive and negative reviews. Your task is to:\n","+ Construct input-output pairs corresponding to the current word and a word in the context window\n","+ Implement forward and backward propagation with parameter updates for Skip-Gram model\n","+ Train the model\n","+ Test it on word analogies and sentiment analysis task\n","\n","## General remarks\n","Do not add or modify any code outside of the following comment blocks, or where otherwise explicitly stated.\n","\n","``` python\n","##########################################################\n","# YOUR CODE HERE\n","...\n","##########################################################\n","```\n","After you fill in all the missing code, restart the kernel and re-run all the cells in the notebook.\n","\n","The following things are **NOT** allowed:\n","- Using additional `import` statements\n","- Copying / reusing code from other sources (e.g. code by other students)\n","\n","If you plagiarise even for a single project task, you won't be eligible for the bonus this semester."]},{"cell_type":"markdown","metadata":{"id":"XXfNbKHcT24y","colab_type":"text"},"source":["# 1. Load data (5 pts)\n","\n","We'll be working with a subset of reviews for restaurants in Las Vegas. The reviews that we'll be working with are either 1-star or 5-star. You can download the used data set (`task03_data.npy`) from:\n","\n","* ([download link](https://syncandshare.lrz.de/getlink/fiQWKmLp3RmNbJEoLtBr3DFu/task03_data.npy)) the preprocessed set of 1-star and 5-star reviews "]},{"cell_type":"code","metadata":{"id":"GfX9h5fjT24z","colab_type":"code","colab":{}},"source":["data = np.load(\"task03_data.npy\", allow_pickle=True)\n","reviews_1star = [[x.lower() for x in s] for s in data.item()[\"reviews_1star\"]]\n","reviews_5star = [[x.lower() for x in s] for s in data.item()[\"reviews_5star\"]]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lrJKphKvT25B","colab_type":"text"},"source":["We generate the vocabulary by taking the top 500 words by their frequency from both positive and negative sentences. We could also use the whole vocabulary, but that would be slower."]},{"cell_type":"code","metadata":{"id":"5KxvGMNnT25E","colab_type":"code","colab":{}},"source":["vocabulary = [x for s in reviews_1star + reviews_5star for x in s]\n","vocabulary, counts = zip(*Counter(vocabulary).most_common(500))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x3QKi-HmT25P","colab_type":"code","colab":{}},"source":["VOCABULARY_SIZE = len(vocabulary)\n","EMBEDDING_DIM = 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b77v_QTET25d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1593951145170,"user_tz":-120,"elapsed":643,"user":{"displayName":"Clara Leung","photoUrl":"","userId":"02206143411713870524"}},"outputId":"7d628d0e-a984-4b88-b8cf-85ee1cec1588"},"source":["print('Number of positive reviews:', len(reviews_1star))\n","print('Number of negative reviews:', len(reviews_5star))\n","print('Number of unique words:', VOCABULARY_SIZE)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of positive reviews: 1000\n","Number of negative reviews: 2000\n","Number of unique words: 500\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VkcwiEj-T25q","colab_type":"text"},"source":["You have to create two dictionaries: `word_to_ind` and `ind_to_word` so we can go from text to numerical representation and vice versa. The input into the model will be the index of the word denoting the position in the vocabulary."]},{"cell_type":"code","metadata":{"id":"Svobf0ctT25s","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1593951172074,"user_tz":-120,"elapsed":744,"user":{"displayName":"Clara Leung","photoUrl":"","userId":"02206143411713870524"}},"outputId":"b2f74c3a-85ae-495d-8755-9514c90f7967"},"source":["\"\"\"\n","Implement\n","---------\n","word_to_ind: dict\n","    The keys are words (str) and the value is the corresponding position in the vocabulary\n","ind_to_word: dict\n","    The keys are indices (int) and the value is the corresponding word from the vocabulary\n","ind_to_freq: dict\n","    The keys are indices (int) and the value is the corresponding count in the vocabulary\n","\"\"\"\n","\n","##########################################################\n","# YOUR CODE HERE\n","# def word_to_ind(word):\n","#   return vocabulary.index(word)\n","\n","# def ind_to_word(index):\n","#   return vocabulary[index]\n","\n","# def ind_to_freq(index):\n","#   return counts[index]\n","word_to_ind = {}\n","ind_to_word = {}\n","ind_to_freq = {}\n","for i in range(VOCABULARY_SIZE):\n","  word = vocabulary[i]\n","  word_to_ind[word] = i\n","  ind_to_word[i] = word\n","  ind_to_freq[i] = counts[i]\n","\n","print('First 10 words:', vocabulary[0:10])\n","print('First 10 counts:', counts[0:10])\n","print('Word \\\"%s\\\" is at position %d appearing %d times' % \n","      (ind_to_word[word_to_ind['for']], word_to_ind['for'], ind_to_freq[word_to_ind['for']]))\n","##########################################################"],"execution_count":null,"outputs":[{"output_type":"stream","text":["First 10 words: ('the', 'and', 'i', 'a', 'to', 'was', 'of', 'for', 'it', 'is')\n","First 10 counts: (2017, 1393, 977, 967, 943, 754, 569, 502, 495, 492)\n","Word \"for\" is at position 7 appearing 502 times\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MFfEZtoFT25_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593951174622,"user_tz":-120,"elapsed":592,"user":{"displayName":"Clara Leung","photoUrl":"","userId":"02206143411713870524"}},"outputId":"a9c66274-319a-4429-f208-47df96fd6f06"},"source":["print('Word \\\"%s\\\" is at position %d appearing %d times' % \n","      (ind_to_word[word_to_ind['the']], word_to_ind['the'], ind_to_freq[word_to_ind['the']]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Word \"the\" is at position 0 appearing 2017 times\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xayTiJpjT26L","colab_type":"text"},"source":["# 2. Create word pairs (10pts)\n","\n","We need all the word pairs $\\{ x_i, x_j \\}$, where $x_i$ is the current word and $x_j$ is from its context window. These will correspond to input-output pairs. We want them to be represented numericaly so you should use `word_to_ind` dictionary."]},{"cell_type":"code","metadata":{"id":"mv56cOrkT26N","colab_type":"code","colab":{}},"source":["def get_window(sentence, window_size):\n","    sentence = [x for x in sentence if x in vocabulary]\n","    pairs = []\n","\n","    \"\"\"\n","    Iterate over all the sentences\n","    Take all the words from (i - window_size) to (i + window_size) and save them to pairs\n","    \n","    Parameters\n","    ----------\n","    sentence: list\n","        A list of sentences, each sentence containing a list of words of str type\n","    window_size: int\n","        A positive scalar\n","        \n","    Returns\n","    -------\n","    pairs: list\n","        A list of tuple (word index, word index from its context) of int type\n","    \"\"\"\n","\n","    ##########################################################\n","    # YOUR CODE HERE\n","    # print('sentence', sentence)\n","    sentence_len = len(sentence)\n","    for i in range(sentence_len):\n","      word = sentence[i]\n","      # print('new word:', word)\n","      for j in range(window_size):\n","        l = j + 1\n","\n","        if (i - l) >= 0:\n","          # print((sentence[i - l], word_to_ind[sentence[i - l]], i - l))\n","          # pairs.append((sentence[i - l], word_to_ind[sentence[i - l]], i - l))\n","          pairs.append((word_to_ind[sentence[i - l]], i - l))\n","\n","        if (i + l) < sentence_len:\n","          # print((sentence[i + l], word_to_ind[sentence[i + l]], i + l))\n","          # pairs.append((sentence[i + l], word_to_ind[sentence[i + l]], i + l))\n","          pairs.append((word_to_ind[sentence[i + l]], i + l))\n","    ##########################################################\n","\n","    return pairs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_PNxpjtZT26W","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1593951187546,"user_tz":-120,"elapsed":717,"user":{"displayName":"Clara Leung","photoUrl":"","userId":"02206143411713870524"}},"outputId":"c3dc66b9-18d0-4b08-a882-3d8987c36b4f"},"source":["data = []\n","for x in reviews_1star + reviews_5star:\n","    data += get_window(x, window_size=3)\n","data = np.array(data)\n","\n","print('First 5 pairs:', data[:5].tolist())\n","print('Total number of pairs:', data.shape[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["First 5 pairs: [[6, 1], [64, 2], [320, 3], [10, 0], [64, 2]]\n","Total number of pairs: 152322\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gmvyIejAT26f","colab_type":"text"},"source":["We calculate a weighting score to counter the imbalance between the rare and frequent words. Rare words will be sampled more frequently. See https://arxiv.org/pdf/1310.4546.pdf"]},{"cell_type":"code","metadata":{"id":"MWOLVA_jT26h","colab_type":"code","colab":{}},"source":["probabilities = [1 - np.sqrt(1e-3 / ind_to_freq[x]) for x in data[:,0]]\n","probabilities /= np.sum(probabilities)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_h7no1dfT26s","colab_type":"text"},"source":["# 3. Model definition (45 pts)\n","\n","In this part you should implement forward and backward propagation together with update of the parameters i.e.:\n","+ One-hot encoding of the words(5 pts)\n","+ Loss implementation & computation (10 pts)\n","+ Softmax (5 pts)\n","+ Forward pass (10 pts)\n","+ Backward pass (10 pts)\n","+ Parameter update (5 pts)"]},{"cell_type":"code","metadata":{"id":"RgUw0Nx9T26u","colab_type":"code","colab":{}},"source":["class Embedding():\n","    def __init__(self, N, D, seed=None):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        N: int\n","            Number of unique words in the vocabulary\n","        D: int\n","            Dimension of the word vector embedding\n","        seed: int\n","            Sets the random seed, if omitted weights will be random\n","        \"\"\"\n","\n","        self.N = N\n","        self.D = D\n","        \n","        self.init_weights(seed)\n","    \n","    def init_weights(self, seed=None):\n","        if seed is not None:\n","            np.random.seed(seed)\n","\n","        \"\"\"\n","        We initialize weight matrices U and V of dimension (D, N) and (N, D) respectively\n","        \"\"\"\n","        self.U = np.random.normal(0, np.sqrt(2. / self.D / self.N), (self.D, self.N))\n","        self.V = np.random.normal(0, np.sqrt(2. / self.D / self.N), (self.N, self.D))\n","\n","    def one_hot(self, x, N):\n","        \"\"\"\n","        Given a vector returns a matrix with rows corresponding to one-hot encoding\n","        \n","        Parameters\n","        ----------\n","        x: array\n","            M-dimensional vector containing integers from [0, N]\n","        N: int\n","            Number of posible classes\n","        \n","        Returns\n","        -------\n","        one_hot: array\n","            (N, M) matrix where each column is N-dimensional one-hot encoding of elements from x \n","        \"\"\"\n","\n","        ##########################################################\n","        # YOUR CODE HERE\n","        one_hot = np.zeros((N, x.shape[0]))\n","        #print('x shape', x.shape)\n","        one_hot[x, np.arange(x.size)] = 1\n","        ##########################################################\n","\n","        assert one_hot.shape == (N, x.shape[0])\n","        return one_hot\n","\n","    def loss(self, y, prob):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        y: array\n","            (N, M) matrix of M samples where columns are one-hot vectors for true values\n","        prob: array\n","            (N, M) column of M samples where columns are probability vectors after softmax\n","\n","        Returns\n","        -------\n","        loss: int\n","            Cross-entropy loss calculated as: 1 / M * sum_i(sum_j(y_ij * log(prob_ij)))\n","        \"\"\"\n","\n","        ##########################################################\n","        # YOUR CODE HERE\n","        M = prob.shape[1]\n","        # loss = - (1. / M) * np.sum(np.sum(y * np.log(prob), axis=0, keepdims=True), axis=1) \n","        # print('original', loss.shape, loss)\n","        loss = - 1 / M * (y * np.log(prob)).sum()\n","        # print('my', loss.shape, loss)\n","        ##########################################################\n","        \n","        return loss\n","    \n","    def softmax(self, x, axis):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        x: array\n","            A non-empty matrix of any dimension\n","        axis: int\n","            Dimension on which softmax is performed\n","            \n","        Returns\n","        -------\n","        y: array\n","            Matrix of same dimension as x with softmax applied to 'axis' dimension\n","        \"\"\"\n","        \n","        ##########################################################\n","        # YOUR CODE HERE\n","        # scipy.special.softmax(x, axis = axis)\n","        e = np.exp(x - np.max(x))\n","        y =  e / e.sum(axis = axis)\n","\n","        #e_x = np.exp(x - np.max(x))\n","        #return e_x / e_x.sum(axis=axis)\n","        ##########################################################\n","\n","        return y\n","    \n","    def step(self, x, y, learning_rate=1e-3):\n","        \"\"\"\n","        Performs forward and backward propagation and updates weights\n","        \n","        Parameters\n","        ----------\n","        x: array\n","            M-dimensional mini-batched vector containing input word indices of int type\n","        y: array\n","            Output words, same dimension and type as 'x'\n","        learning_rate: float\n","            A positive scalar determining the update rate\n","            \n","        Returns\n","        -------\n","        loss: float\n","            Cross-entropy loss\n","        d_U: array\n","            Partial derivative of loss w.r.t. U\n","        d_V: array\n","            Partial derivative of loss w.r.t. V\n","        \"\"\"\n","        \n","        # Input transformation\n","        \"\"\"\n","        Input is represented with M-dimensional vectors\n","        We convert them to (N, M) matrices such that columns are one-hot \n","        representations of the input\n","        \"\"\"\n","        x = self.one_hot(x, self.N)\n","        y = self.one_hot(y, self.N)\n","\n","        \n","        # Forward propagation\n","        \"\"\"\n","        Returns\n","        -------\n","        embedding: array\n","            (D, M) matrix where columns are word embedding from U matrix\n","        logits: array\n","            (N, M) matrix where columns are output logits\n","        prob: array\n","            (N, M) matrix where columns are output probabilities\n","        \"\"\"\n","        \n","        ##########################################################\n","        # YOUR CODE HERE\n","        embedding = np.dot(self.U, x)\n","        logits = np.dot(self.V, embedding)\n","        prob = self.softmax(logits, axis = 0)\n","\n","        # print(embedding)\n","        # print(logits)\n","        # print('x', x.shape)\n","        # print('embedding', (self.D, x.shape[1]), embedding.shape)\n","        # print('logits', (self.N, x.shape[1]), logits.shape)\n","        # print('prob', (self.N, x.shape[1]), prob.shape)\n","        # print(prob)\n","        ##########################################################\n","\n","        assert embedding.shape == (self.D, x.shape[1])\n","        assert logits.shape == (self.N, x.shape[1])\n","        assert prob.shape == (self.N, x.shape[1])\n","    \n","        # Loss calculation\n","        \"\"\"\n","        Returns\n","        -------\n","        loss: int\n","            Cross-entropy loss using true values and probabilities\n","        \"\"\"\n","        ##########################################################\n","        # YOUR CODE HERE\n","        loss = self.loss(y, prob)\n","        ##########################################################\n","\n","        # Backward propagation\n","        \"\"\"\n","        Returns\n","        -------\n","        d_U: array\n","            (D, N) matrix of partial derivatives of loss w.r.t. U\n","        d_V: array\n","            (N, D) matrix of partial derivatives of loss w.r.t. V\n","        \"\"\"\n","        ##########################################################\n","        # YOUR CODE HERE\n","        #print('y:', y)\n","        dL_dZ = prob - y\n","        M = embedding.shape[1]\n","        d_V = np.dot(dL_dZ, embedding.T)\n","        #d_U = np.dot(x, np.dot(dL_dZ.T, self.V)).T\n","        #d_U = self.V.T.dot(x).dot(dL_dZ.T)\n","        d_U = self.V.T.dot(dL_dZ).dot(x.T)\n","\n","        #d_U = np.zeros(self.U.shape)\n","        #d_V = np.zeros(self.V.shape)\n","        # for i in range(len(x.shape[1])): # x.shape[1] = M\n","        #for i in range(x.shape[1]): # x.shape[1] = M\n","        #    inp = x[:, i] # inp.shape = N\n","        #    h = np.dot(self.U.T, inp) # D x 1\n","        #    u = np.dot(self.V.T, h) # N x 1\n","        #    EI = np.sum(prob[:, i] - y[:, i], axis=0) # N x 1\n","        #    d_v = EI.dot(inp.reshape(-1,1).dot(v))\n","        #    print('d_V', d_V.shape)\n","        ##########################################################\n","        \n","        assert d_V.shape == (self.N, self.D)\n","        assert d_U.shape == (self.D, self.N)\n","\n","        # Update the parameters\n","        \"\"\"\n","        Updates the weights with gradient descent such that W_new = W - alpha * dL/dW, \n","        where alpha is the learning rate and dL/dW is the partial derivative of loss w.r.t. \n","        the weights W\n","        \"\"\"\n","        ##########################################################\n","        # YOUR CODE HERE\n","        self.U = self.U - (learning_rate * d_U)\n","        self.V = self.V - (learning_rate * d_V)\n","        ##########################################################\n","\n","        return loss, d_U, d_V"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0uFBfQFaT265","colab_type":"text"},"source":["## 3.1 Gradient check\n","\n","The following code checks whether the updates for weights are implemented correctly. It should run without an error."]},{"cell_type":"code","metadata":{"id":"kLZNJVI5T268","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":746},"executionInfo":{"status":"ok","timestamp":1593953753709,"user_tz":-120,"elapsed":1519,"user":{"displayName":"Clara Leung","photoUrl":"","userId":"02206143411713870524"}},"outputId":"26b432f2-0de0-4d57-bbfa-136c20d56085"},"source":["def get_loss(model, old, variable, epsilon, x, y, i, j):\n","    delta = np.zeros_like(old)\n","    delta[i, j] = epsilon\n","\n","    model.init_weights(seed=132) # reset weights\n","    setattr(model, variable, old + delta) # change one weight by a small amount\n","    loss, _, _ = model.step(x, y) # get loss\n","\n","    return loss\n","\n","def gradient_check_for_weight(model, variable, i, j, k, l):\n","    x, y = np.array([i]), np.array([j]) # set input and output\n","    \n","    old = getattr(model, variable)\n","    \n","    model.init_weights(seed=132) # reset weights\n","    _, d_U, d_V = model.step(x, y) # get gradients with backprop\n","    grad = { 'U': d_U, 'V': d_V }\n","    \n","    eps = 1e-4\n","    loss_positive = get_loss(model, old, variable, eps, x, y, k, l) # loss for positive change on one weight\n","    loss_negative = get_loss(model, old, variable, -eps, x, y, k, l) # loss for negative change on one weight\n","    \n","    true_gradient = (loss_positive - loss_negative) / 2 / eps # calculate true derivative wrt one weight\n","    print(variable, true_gradient, grad[variable][k, l])\n","    assert abs(true_gradient - grad[variable][k, l]) < 1e-5 # require that the difference is small\n","\n","def gradient_check():\n","    N, D = VOCABULARY_SIZE, EMBEDDING_DIM\n","    model = Embedding(N, D)\n","\n","    # check for V\n","    for _ in range(20):\n","        i, j, k = [np.random.randint(0, d) for d in [N, N, D]] # get random indices for input and weights\n","        gradient_check_for_weight(model, 'V', i, j, i, k)\n","\n","    # check for U\n","    for _ in range(20):\n","        i, j, k = [np.random.randint(0, d) for d in [N, N, D]]\n","        gradient_check_for_weight(model, 'U', i, j, k, i)\n","\n","    print('Gradients checked - all good!')\n","\n","gradient_check()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["V -2.3156294659543164e-05 -2.3145718868858642e-05\n","V 2.1075519107682794e-05 2.1071602531151087e-05\n","V 2.10754969032223e-05 2.1071602531151087e-05\n","V 2.1075479139653908e-05 2.1071602531151087e-05\n","V 2.1075452494301317e-05 2.1071602531151087e-05\n","V 2.1075434730732923e-05 2.1071602531151087e-05\n","V 2.107540808538033e-05 2.1071602531151087e-05\n","V 2.107538588091984e-05 2.1071602531151087e-05\n","V 2.1075368117351445e-05 2.1071602531151087e-05\n","V 2.1075345912890953e-05 2.1071602531151087e-05\n","V 2.107532370843046e-05 2.1071602531151087e-05\n","V 2.107529706307787e-05 2.1071602531151087e-05\n","V 2.1075274858617377e-05 2.1071602531151087e-05\n","V 2.1075257095048983e-05 2.1071602531151087e-05\n","V 2.107523044969639e-05 2.1071602531151087e-05\n","V 2.1075212686127998e-05 2.1071602531151087e-05\n","V 2.1075186040775407e-05 2.1071602531151087e-05\n","V 2.1075163836314914e-05 2.1071602531151087e-05\n","V 2.107514163185442e-05 2.1071602531151087e-05\n","V 2.107511942739393e-05 2.1071602531151087e-05\n","U -0.006036175506096697 -0.006036175313788938\n","U -0.006036179831525601 -0.006036175313788938\n","U -0.006036184152513613 -0.006036175313788938\n","U -0.006036188473501625 -0.006036175313788938\n","U -0.0060361927944896365 -0.006036175313788938\n","U -0.0060361971199185405 -0.006036175313788938\n","U -0.006036201440906552 -0.006036175313788938\n","U -0.006036205761894564 -0.006036175313788938\n","U -0.006036210082882576 -0.006036175313788938\n","U -0.00603621440831148 -0.006036175313788938\n","U -0.006036218729299492 -0.006036175313788938\n","U -0.006036223054728396 -0.006036175313788938\n","U -0.0060362273757164076 -0.006036175313788938\n","U -0.006036231696704419 -0.006036175313788938\n","U -0.006036236017692431 -0.006036175313788938\n","U -0.006036240343121335 -0.006036175313788938\n","U -0.006036244664109347 -0.006036175313788938\n","U -0.006036248985097359 -0.006036175313788938\n","U -0.006036253310526263 -0.006036175313788938\n","U -0.006036257631514275 -0.006036175313788938\n","Gradients checked - all good!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Uku0wXGXT27F","colab_type":"text"},"source":["# 4. Training"]},{"cell_type":"markdown","metadata":{"id":"LQcaqu1gT27G","colab_type":"text"},"source":["We train our model using stochastic gradient descent. At every step we sample a mini-batch from data and update the weights.\n","\n","The following function samples words from data and creates mini-batches. It subsamples frequent words based on previously calculated probabilities."]},{"cell_type":"code","metadata":{"id":"nLC4lPt_T27H","colab_type":"code","colab":{}},"source":["def get_batch(data, size, prob):\n","    i = np.random.choice(data.shape[0], size, p=prob)\n","    return data[i, 0], data[i, 1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CxyAH2WHT27R","colab_type":"text"},"source":["Training the model can take some time so plan accordingly."]},{"cell_type":"code","metadata":{"id":"VMn23p_3T27T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":284},"executionInfo":{"status":"ok","timestamp":1593955408005,"user_tz":-120,"elapsed":1647249,"user":{"displayName":"Clara Leung","photoUrl":"","userId":"02206143411713870524"}},"outputId":"24c74c07-16e6-42a5-a6ab-a9989f793605"},"source":["np.random.seed(123)\n","model = Embedding(N=VOCABULARY_SIZE, D=EMBEDDING_DIM)\n","\n","losses = []\n","\n","MAX_ITERATIONS = 150000\n","PRINT_EVERY = 10000\n","\n","for i in range(MAX_ITERATIONS):\n","    x, y = get_batch(data, 128, probabilities)\n","    loss, _, _ = model.step(x, y, 1e-2)\n","    losses.append(loss)\n","\n","    if (i + 1) % PRINT_EVERY == 0:\n","        print('Iteration:', i + 1, 'Loss:', np.mean(losses[-PRINT_EVERY:]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Iteration: 10000 Loss: 2.9833665251778196\n","Iteration: 20000 Loss: 2.81655229984457\n","Iteration: 30000 Loss: 2.8025507032656867\n","Iteration: 40000 Loss: 2.7982243510297224\n","Iteration: 50000 Loss: 2.7940718977745878\n","Iteration: 60000 Loss: 2.794366293179324\n","Iteration: 70000 Loss: 2.793146348535563\n","Iteration: 80000 Loss: 2.792433209987576\n","Iteration: 90000 Loss: 2.792025695537039\n","Iteration: 100000 Loss: 2.7918336265615924\n","Iteration: 110000 Loss: 2.7911438089400677\n","Iteration: 120000 Loss: 2.7912506538953226\n","Iteration: 130000 Loss: 2.791206612020503\n","Iteration: 140000 Loss: 2.790938272305264\n","Iteration: 150000 Loss: 2.790298691094442\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IXIHXagvT27d","colab_type":"text"},"source":["The embedding matrix is given by $\\mathbf{U}^T$, where the $i$th row is the vector for $i$th word in the vocabulary."]},{"cell_type":"code","metadata":{"id":"sGvPN8dXT27f","colab_type":"code","colab":{}},"source":["emb_matrix = model.U.T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eSeOH45FT27n","colab_type":"text"},"source":["# 5. Analogies (10 pts)\n","\n","As mentioned before, vectors can keep some language properties like analogies. Given a relation a:b and a query c, we can find d such that c:d follows the same relation. We hope to find d by using vector operations. In this case, finding the real word vector $\\mathbf{u}_d$ closest to $\\mathbf{u}_b - \\mathbf{u}_a + \\mathbf{u}_c$ gives us d. Note that the quality of the analogy results is not expected to be excellent."]},{"cell_type":"code","metadata":{"id":"Ic1isewoT27o","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1593958097900,"user_tz":-120,"elapsed":890,"user":{"displayName":"Clara Leung","photoUrl":"","userId":"02206143411713870524"}},"outputId":"67aec5f5-0519-4323-d254-2b7940576998"},"source":["triplets = [['go', 'going', 'come'], ['look', 'looking', 'come'], ['i', 'you', 'we'], \n","            ['what', 'that', 'when'], ['find', 'found', 'enjoy']]\n","\n","for triplet in triplets:\n","    a, b, c = triplet\n","\n","    \"\"\"\n","    Returns\n","    -------\n","    candidates: list\n","        A list of 5 closest words, measured with cosine similarity, to the vector u_b - u_a + u_c\n","    \"\"\"\n","    ##########################################################\n","    # YOUR CODE HERE\n","    target_emb = emb_matrix[word_to_ind[b]] - emb_matrix[word_to_ind[a]] + emb_matrix[word_to_ind[c]]\n","\n","    def cosine_similarity(x):\n","      return spatial.distance.cosine(x, target_emb)\n","\n","    # applied cosine_similarity by each row, i.e. each word embedding, in emb_martix\n","    cs_matrix = np.apply_along_axis(cosine_similarity, axis = 1, arr = emb_matrix) \n","    candidates_ind = cs_matrix.argsort()[::-1][:5]\n","\n","    candidates = []\n","    for i in candidates_ind:\n","      candidates.append(ind_to_word[i])\n","    ##########################################################\n","    \n","    print('%s is to %s as %s is to [%s]' % (a, b, c, '|'.join(candidates)))   \n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["go is to going as come is to [won't|enough|then|items|end]\n","look is to looking as come is to [look|day|stars|glass|terrible]\n","i is to you as we is to [water|give|area|too|look]\n","what is to that as when is to [what|tea|told|rolls|wasn't]\n","find is to found as enjoy is to [its|area|oh|wrong|getting]\n"],"name":"stdout"}]}]}